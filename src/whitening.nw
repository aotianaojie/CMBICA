% -*- mode: Noweb; noweb-code-mode: python-mode -*-

\subsection{Whitening the Noise in the Maps}
\label{sec:whitening}

The so-called \emph{whitening step} of the \FastICA{} algorithm aims to improve
the statistical properties of the signal needed by the algorithm itself by
using the principles of Principal Component Analysis (PCA).

Principal Component Analysis is a special kind of orthogonal transformation
(rotation) that, when applied to a set of $n$ vectors $\{e_i\}_{i=1}^n$,
produces a new set of $n$ vectors $\{e_i'\}_{i=1}^n$ sorted in decreasing order
according to their variance. The point is that the most interesting stuff has
been moved to the first $m$ vectors (with $m$ being some number smaller than
$n$ so that $e_{m+1}$ is the first vector that can be considered ``almost''
constant).  Thus, PCA is a way to reduce the dimensionality of a dataset by
``selecting'' preferred directions and throwing away those components that do
not vary too much.

\begin{figure}[tbf]
    \centering
    \includegraphics[width=0.45\textwidth]{correlation-dmr.pdf}
    \includegraphics[width=0.45\textwidth]{correlation-dmr-whitened.pdf}
    \caption{\label{fig:correlationDMR} Left: correlation between the (Healpix)
sky maps produced by COBE DMR at 31\,GHz and 53\,GHz. The correlation
coefficient $r$ is 0.72. Right: the same maps after having been whitened: now
the correlation is nearly zero (the covariance matrix of the two maps is the $2
\times 2$ identity matrix).}
\end{figure}

One can easily see that the sky shows some correlation at different
frequencies: take for instance fig.~\ref{fig:correlationDMR}, which visually
shows the correlation between the 31\,GHz and the 53\,GHz channels of COBE-DMR.
This means that the $n \times m$ matrix $X$ containing data from the $m$ maps
has a covariance matrix which has nonzero off-diagonal elements.
\cmbica{} uses PCA to transform $X$ into a new matrix $X'$ of the same size
which has the covariance matrix equal to unity. Doing so is important for the
application of the \FastICA{} algorithm.

The method used by \cmbica{} to transform $X$ into $X'$ is a standard technique
and is described in \citet[ch.~1]{JolliffePCA}. It requires to build a $n
\times n$ matrix with the eigenvectors of the covariance matrix (a symmetric
real matrix -- this grants that the eigenvalues are all real) sorted according
to the absolute value of their eigenvalues in decreasing order. Such matrix is
orthogonal and therefore represents a base change: in our case it is equal to
the ``whitening matrix'' which does the PCA. Mathematically, if $\Sigma$ is the
covariance matrix of the noise and $C$ the covariance matrix of the map (i.e.\
signal \emph{and} noise), the orthogonal matrix which represents the base
change is $(C - \Sigma)^{1/2}$.  Therefore, if the maps are stored in matrix
$X$, PCA produces a new set of maps $X'$ according to the following formula:
\begin{equation}
\label{eq:mapPCA}
X' = (C - \Sigma)^{-1/2} X.
\end{equation}
The covariance matrix $\Sigma'$ for the new maps is
\begin{equation}
\label{eq:sigmaPCA}
\Sigma' = (C - \Sigma)^{-1/2} \Sigma (C - \Sigma)^{1/2},
\end{equation}
It is easy to prove that $X'$ has a covariance matrix equal to unity.

We start defining two basic functions: the first one,
[[remove_monopole_from_maps]], removes the average value from a set of maps and
returns a $n\times m$ matrix, with $n$ being the number of pixels per map and
$m$ the number of components:
<<Function definitions>>=
def remove_monopole_from_maps (maps):
    'Remove the monopole from every map and return a numpy array.'
    map_matrix = np.array (maps)
    for idx in xrange (len (maps)):
        map_matrix[idx,:] = maps[idx] - np.mean (maps[idx])

    return map_matrix
@ %def remove_monopole_from_maps
The test case for this function is provided by the following method (to be
incorporated in a test class derived from FastICABaseTestCase, in order to have
the proper definition for [[self.x1]]\ and [[self.x2]], see below):
<<Test for [[remove_monopole]]>>=
def test_remove_monopole (self):
    result = remove_monopole_from_maps (np.array ([self.x1,
						   self.x2]))
    self.assertAlmostEqual (np.mean (result[0,:]), 0.0)	
    self.assertAlmostEqual (np.mean (result[1,:]), 0.0)	
@ %def test_remove_monopole

The second function, [[get_ordered_eig]], returns a tuple containing the
eigenvalues and eigenvectors of the real symmetric matrix [[x]], sorted
accorted to their decreasing magnitude:
<<Function definitions>>=
def get_ordered_eig (x):
    '''Return (V, D) where V and D are the eigenvalues and
    eigenvectors of `x' (a real symmetrix matrix). They are
    sorted in decreasing order of the eigenvalues' magnitudes.'''

    (evals, evects) = linalg.eigh (x)

    # Sort the eigenvalues in decreasing order
    perm = np.argsort (-np.abs (evals))
    return (evals[perm], evects[:,perm])
@ %def get_ordered_eig
Since the [[linalg]]\ module is part of the [[scipy]]\ package, we must import it
at the beginning of the main program:
<<Import statements>>=
from scipy import linalg
@

The test case for [[get_ordered_eig]] uses a very simple matrix for the purpose:
\[
A = \begin{pmatrix}
-2 & 3 \\
 3 & -10
\end{pmatrix},
\]
which has -1 and -11 as eigenvalues. The eigenvectors are not uniquely
determined\footnote{If $\vect{v}$ is an eigenvector whose eigenvalue is
$\lambda$, then any $\alpha \vect{v}$ will be the same for any real number
$\alpha \not= 0$.}, so we cannot compare the result of [[get_ordered_eig]] with
a fixed vector.  Therefore, to write the test we will simply stick to the
definition of ``eigenvector'': $\vect{v}$ is an eigenvector of $A$ with
eigenvalue $\lambda$ if $A\cdot\vect{v} = \lambda\vect{v}$, that is if
\[
A\cdot\vect{v} - \lambda \vect{v} = 0.
\]
The following code implements the formula above to check the correctness of the
eigenvectors:
<<Test for [[get_ordered_eig]]>>=
def test_get_ordered_eig (self):
    # Check the eigenvalues
    A = np.array ([[-2, 3], [3, -10]])
    result = get_ordered_eig (A)
    self.assertEqual (len (result), 2)
    assert np.allclose (result[0], np.array([-11.,  -1.]))

    # Check the eigenvectors
    self.assertEqual (len (result[1]), 2)
    eigenvects = result[1] * np.sqrt (10)
    print "eigenvectors = ", result[1]
    for i in (0, 1):
	v = result[1][:,i]
	a_times_v = np.dot (A, v)
	assert np.allclose (a_times_v - result[0][i] * v,
			    np.zeros (v.size))
@ %def test_get_ordered_eig
As we did above for [[test_remove_monopole]], we are going to make this a
method of a class derived from [[FastICABaseTestCase]]\ (this motivates the
[[self]] parameter).

We have now all the necessary pieces to write [[whiten_maps]]. Note that we are
not going to call [[remove_monopole_from_maps]]\ within [[whiten_maps]], since
we are going to use the matrix of zero-mean maps also outside [[whiten_maps]].
Therefore, we assume here that [[map_matrix]]\ is the result of a previous call
to [[remove_monopole_from_maps]]:
<<Function definitions>>=
def whiten_maps (map_matrix):
    '''Given a matrix representing `m' zero-mean maps, return a pair containing
    the whitening/dewhitening matrices.'''

    <<Find the sorted eigenvalues of the covariance matrix for [[map_matrix]]>>
    <<Compute [[whitening_matrix]] and [[dewhitening_matrix]]>>
    return (whitening_matrix, dewhitening_matrix)
@ %def whiten_maps
\noindent where [[map_matrix]]\ is the $X$ in eq.~\eqref{eq:mapPCA}, a $n\times
m$ matrix containing the $n$ maps each with $m = 12\times \mathrm{NSIDE}^2$
pixels.

Estimating the covariance matrix $C$ is a simple matter of multiplying $X$ for
$X^t$ and scaling by the number of maps. We can use the BLAS function [[dgemm]]
to do everything in one operation. Having $C$ we can look for its eigenvalues
and eigenvectors:
<<Find the sorted eigenvalues of the covariance matrix for [[map_matrix]]>>=
cov_matrix = linalg.fblas.dgemm (1.0 / map_matrix.shape[1],
                                 map_matrix,
                                 map_matrix,
                                 trans_b = True)

(evals, evects) = get_ordered_eig (cov_matrix)

log.info ("Principal components: %s" % str (evals))
@

Finally, we compute the whitening and dewhitening matrices $(C - \Sigma)^{1/2}$
and $(C - \Sigma)^{-1/2}$. The former is used to whiten the maps.
<<Compute [[whitening_matrix]] and [[dewhitening_matrix]]>>=
sqrt_evals = np.sqrt (np.abs (evals))
whitening_matrix = np.dot (evects,
                           np.dot (np.diag (1.0 / sqrt_evals),
                                   np.transpose (evects)))
dewhitening_matrix = np.dot (evects,
                             np.dot (np.diag (sqrt_evals),
                                     np.transpose (evects)))
@ %def whitening_matrix dewhitening_matrix

The test case for [[whiten_maps]] is reported here. It simply checks that the
covariance matrix of the transformed signal matrix is unity:
<<Test for [[whiten_maps]]>>=
def test_whiten_maps (self):
    result = whiten_maps (self.mixed_signals)
    print "cov (self.mixed_signals) = ", np.cov (self.mixed_signals)
    print "result[0].shape = ", result[0].shape
    print "self.mixed_signals.shape = ", self.mixed_signals.shape
    whitened_sigs = np.dot (result[0], self.mixed_signals)
    print "whitened_sigs.shape = ", whitened_sigs.shape
    print "cov (whitened_sigs) = ", np.cov (whitened_sigs)
    assert np.allclose (np.cov (whitened_sigs),
			np.identity (2))
@

\subsubsection{Definition of a Test Case}

Using [[FastICABaseTestCase]] (defined in sect.~\ref{sec:test}) as the base
class, we provide here the implementation of a test class for the
[[whiten_maps]] function:
<<Test cases>>=
class WhitenTestCase (FastICABaseTestCase):
    <<Test for [[remove_monopole]]>>
    <<Test for [[get_ordered_eig]]>>
    <<Test for [[whiten_maps]]>>
@ %def WhitenTestCase
where we have simply put together the test cases implemented in this section.
